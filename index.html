<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="">
  <meta property="og:image" content="" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Visionary Co-Driver</title>

  <!-- Google tag (gtag.js) -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-E0GWRL87RE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-E0GWRL87RE');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://everloom-129.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="http://www.idi.zju.edu.cn/en/home">
            International Design Institute of Zhejiang University
          </a>
          <a class="navbar-item" href="https://www.grasp.upenn.edu/">
            GRASP Lab at University of Pennsylvania
          </a>
          <a class="navbar-item" href="https://www.hkust-gz.edu.cn/">
            HKUST(Guanhou)
          </a>
          <a class="navbar-item" href="http://ckc.zju.edu.cn/ckcen/">
            Chu Kochen Honors College of Zhejiang University
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero" style="padding-bottom: 1rem;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Visionary Co-Driver: Enhancing Driver Perception of Potential Risks with LLM and HUD</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://person.zju.edu.cn/en/0617496#0">Wei Xiang</a><sup>*</sup>,</span>
            <span class="author-block">
              Ziyue Le<sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://everloom-129.github.io/">Jie Wang</a><sup>†</sup>,</span>
            <span class="author-block">Yingying Huang<sup>*</sup>,</span>
            <span class="author-block">Qi Zheng<sup>‡</sup>,</span>
            <span class="author-block">Tianyi Zhang<sup>§</sup>,</span>
            <span class="author-block">An Zhao<sup>§</sup>,</span>
            <span class="author-block">
              and <a href="https://person.zju.edu.cn/en/lingyun">Lingyun Sun</a><sup>*</sup></span>
          </div>
          <div class="is-size-6 has-text-grey">
            <span class="author-block"><sup>*</sup>International Design Institute, Zhejiang University</span>
            <span class="author-block"><sup>†</sup>GRASP Lab, University of Pennsylvania</span>
            <span class="author-block"><sup>‡</sup>The Hong Kong University of Science and Technology (Guangzhou)</span>
            <span class="author-block"><sup>§</sup>Chu Kochen Honors College, Zhejiang University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2511.14233"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href="https://openreview.net/pdf?id=djJLvSur1jZ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!--Poster Link -->
              <span class="link-block">
                <a target="_blank" href="./Visionary-CoDriver-Poster.jpg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>            
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/Everloom-129/Visionary-CoDriver"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size:125%;">
            Drivers' perception of risky situations has always been a challenge in driving. Existing risk-detection methods excel at identifying collisions but face challenges in assessing the behavior of road users in non-collision situations. This paper introduces Visionary Co-Driver, a system that leverages large language models (LLMs) to identify non-collision roadside risks and alert drivers based on their eye movements. Specifically, the system combines video processing algorithms and LLMs to identify potentially risky road users. These risks are dynamically indicated on an adaptive heads-up display interface to enhance drivers' attention. A user study with 41 drivers confirms that Visionary Co-Driver improves drivers' risk perception and supports their recognition of roadside risks.
          </p>
          <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; background: #000; border-radius: 8px;">
            <video style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; border-radius: 8px;"
                   controls
                   playsinline
                   preload="metadata">
                <source src="static/videos/CHI_video.mp4" type="video/mp4">
                Your browser does not support HTML5 video playback.
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section" style="padding-top: 0rem; padding-bottom: 0rem; margin-top: -2rem;">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <!-- Teaser Video Placeholder -->
         
        <div style="margin-bottom: 2rem;">
          <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; background: #000; border-radius: 8px;">
            <video style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; border-radius: 8px;"
                   controls
                   playsinline
                   preload="metadata">
                <source src="static/videos/notebookLM-video.mp4" type="video/mp4">
                Your browser does not support HTML5 video playback.
            </video>
          </div>
        </div>
        <!-- TLDR Blurb -->
        <div class="content has-text-justified">
          <p style="font-size:125%;">
            TL;DR: We propose a GOFE way to build a perception system for text-only-LLMs understanding driving scenario and enhance driver perception in a human-centric design. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p style="font-size:125%;">
            Waiting for further information
          </p>
          <div style="margin-top: 16px;">
            <img
              src="static/images/Overall1.png"
              alt="Visionary Co-Driver Overall Diagram"
              style="display: block; width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.08);"
              loading="lazy"
            />
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">User Study</h2>
        <div class="content has-text-justified">
          <p style="font-size:125%;">
            The user study was conducted in a driving simulator environment equipped with a cockpit, a 27-inch LED screen, and Tobii Pro Glasses 3 to record eye-tracking data at 100 Hz. 
            The experiment involved 41 licensed drivers (resulting in 36 valid datasets) who tested three distinct interface conditions—Basic HUD, Vehicle Alert HUD, and the Visionary Co-Driver (VCD)—in a counterbalanced order to reduce learning effects. 
            The procedure consisted of five phases: an introduction, a pre-questionnaire with eye-tracker calibration, a practice phase to familiarize participants with the setup, a formal study phase, and a final session for the User Experience Questionnaire (UEQ) and interviews. During the formal study, participants viewed video clips from BDD and JAAD datasets; the videos were paused at specific intervals to administer risk perception questionnaires based on the Situation Awareness Global Assessment Technique (SAGAT), while cognitive workload was assessed using the Subjective Workload Assessment Technique (SWAT) after each condition.
          </p>
          <div style="margin-top: 16px;">
            <img
              src="static/images/Procedure3.png"
              alt="User Study Procedure"
              style="display: block; width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.08);"
              loading="lazy"
            />
          </div>
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin-top: 24px;">
            <figure style="margin: 0;">
              <img
                src="static/images/Demo.jpg"
                alt="Visionary Co-Driver Demo"
                style="display: block; width: 100%; height: 280px; object-fit: cover; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.08);"
                loading="lazy"
              />
            </figure>
            <figure style="margin: 0;">
              <img
                src="static/images/test_scene.png"
                alt="Visionary Co-Driver Test Scene"
                style="display: block; width: 100%; height: 280px; object-fit: cover; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.08);"
                loading="lazy"
              />
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">

      <a class="icon-link" href="https://github.com/Everloom-129" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Template taken from <a
              href="https://github.com/nerfies/nerfies.github.io">here</a>. GPT-4 also assisted in the development of this site.

          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
// Placeholder for future scripts
</script>

</body>
</html>
